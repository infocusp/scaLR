# CONFIG

# EXPERIMENT 
device: 'cuda' # [Optional][cuda/cpu] device to run the deep learning models on
filepath: '.' # [Required] base file path for all experiments
exp_name: 'scp_test_final' # [Required] experiment name
exp_run: 0 # [Required] experiment run to compare
# Final experiment directory would be: filepath/{exp_name}_{exp_run}/

# DATA
data:
    chunksize: 20000
    # [Optional] Useful for low resources utilization. Will ensure all data is stored with chunks of atmost 'chunksize' samples 
    # [null] for storing data in one file.
    
    # [Optional] To split full anndata dataset file into train/val/test sets
    split_data:
        full_datapath: '/home/biocusp/data/scp_ad/full.h5ad' # full data path, will be split into train, test, and val sets
        split_ratio: [7, 1, 2.5] # [Required] ratio to split full data => train : val : test
        stratify: 'donor_id' # [Optional][column in metadata / null] split stratified on parameter in metadata
        

    # Note if split_data is given, following paths will be overwritten
    # if spilt_data not given, paths are [Required]
    train_datapath: './pipeline_3/feature_selection/train' # training data path
    val_datapath: './pipeline_3/feature_selection/val' # validation data path
    test_datapath: './pipeline_3/feature_selection/test' # testing data path
    
    target: Cell_Type # [Required] target to perform classification on. Must be present as a column_name in adata.obs

# Feature selection technique
feature_selection:
    chunksize: 3000
    # [Required] chunks of features to train the model on iteratively
    
    method_type: feature_chunk # Only method availble now

    # [Required] [name: logistic_classifier/nn]
    # logistic regression: keyword args of logistic regression classifier of sklearn
    #                      can be passes in params
    # nn: params are optional, can include epochs, batch_size, lr- learning rate, l2- L2 weight penalty
    model:
      name: nn
      params:
          epochs: 25

    # [Required]
    # k: number of top features to extract
    # aggregation_strategy: stratergies to obtain top features. only mean implemented now
    top_features_stats:
        k: 4000
        aggregation_strategy: mean # [Required][mean/class weighted density/top (k/n_classes) per class]

    # Will store extracted features subset on disk
    # the storage will be according to data=>chunksize parameter
    store_on_disk: True


# Transformer related implementation still underway
#PREPROCESS & TOKENIZATION
# transformer_preprocessing:
#     value_bin: True
#     n_bins: 51
#     append_cls: True
#     include_zero_gene: False
#     max_len: 3001

# MODEL
# DL model to train [Required]
# type: [Linear]
# hyperparameters:
#    layers: [number of genes...number of classes]. eg. [22858, 2048, 64, 6], dropout, activation, weights_init_zero
# start_checkpoint: if resume_from_checkpoint is true, path to checkpoint, note that this will overwrite above hyperparameters and type
# resume_from_checkpoint: to resume training past checkpoint. [BOOLEAN], default: false
model: 
    type: 'linear'
    hyperparameters:
        layers: [4000, 6]
        dropout: 0
        weights_init_zero: False
    start_checkpoint: null
    resume_from_checkpoint: False

# Transformer related implementation still underway
# model:
#     type: 'transformer'
#     hyperparameters:
#         dim: 128
#         nlayers: 2
#         nheads: 8
#         dropout: 0.2
#         n_cls: 6
#     start_checkpoint: False
#     resume_from_checkpoint: False

# TRAINING
# opt: default Adam
# loss_fn: default CrossEntropy
# batch_size: samples per batch during training
# lr: learning rate
# l2: L2 penalty
# epochs: max number of epochs to train
# callbacks: EARLYSTOPPING, MODELCHECKPOINT, TENSORBOARDLOGGING
#     early_stop_patience: max number of epochs for which validation loss does not improve before stopping
#     early_stop_min_delta: minimum increment in validation loss to count as improvement
#     model_checkpoint_interval: intervals of epoch for which model weights are stored
training:
    opt: 'sgd'
    loss: 'log'
    batch_size: 5000
    lr: 1.0e-3
    l2: 0
    epochs: 100
    callbacks:
        tensorboard_logging: True
        early_stop:
            stop_patience: 3
            stop_min_delta: 1.0e-4
        model_checkpoint:
            checkpoint_interval: 5

# model_checkpoint: [OPTIONAL IF USING WITH PIPELINE AFTER TRAINING]
#     path_to_model_checkpoint: use model checkpoint to load model
# batch_size: evaluation batch_size
# evaluation_metrics: metrics to perform evaluation available:['accuracy', 'report']
# EVALUATION
evaluation:
    model_checkpoint: './pipeline_train_0/best_model'
    batch_size: 5000
    metrics: ['accuracy', 'report']





    