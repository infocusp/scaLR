# CONFIG

# EXPERIMENT
device: 'cuda' # [Optional][cuda/cpu] device to run the deep learning models on
dirpath: '.' # [Required] base directory path for all experiments
exp_name: 'scp_test_eval' # [Required] experiment name
exp_run: 1 # [Required] experiment run to compare
# Final experiment directory would be: dirpath/{exp_name}_{exp_run}/

# DATA
data:
    chunksize: 10000
    # [Optional] Useful for low resources utilization. Will ensure all data is stored with chunks of atmost 'chunksize' samples
    # [null] for storing data in one file.

    # [Optional] To split full anndata dataset file into train/val/test sets
    # split_data:
    #     split_ratio: [7, 1, 2.5] # [Required] ratio to split full data => train : val : test
    #     stratify: 'donor_id' # [Optional][column in metadata / null] split stratified on parameter in metadata

    # [Optional] To perform sample-wise normalization of expression values
    normalize_data: True

    # full_datapath: '/home/biocusp/data/scp_ad/full.h5ad' # full data path, will be split into train, test, and val sets. [Required] for splitting of data

    # Note if split_data is given, following split paths will be overwritten
    # if spilt_data not given, paths are [Required]
    train_datapath: '/home/saiyam/projects/biocusp/scripts/scp_test_final_7/feature_selection/train' # training data path
    val_datapath: '/home/saiyam/projects/biocusp/scripts/scp_test_final_7/feature_selection/val' # validation data path
    test_datapath: '/home/saiyam/projects/biocusp/scripts/scp_test_final_7/feature_selection/test' # testing data path

    target: Cell_Type # [Required] target to perform classification on. Must be present as a column_name in adata.obs

# Feature selection technique
feature_selection:
    chunksize: 5000
    # [Required] chunks of features to train the model on iteratively

    # weight_matrix: 'full_test_0/feature_selection/feature_class_weights.csv'
    # [Optional] path to weight matrix containing weights for all features across all classes
    # If path specified, feature importance algorithm will not work, and directly use weights to perform selection

    method_type: feature_chunk # Only method availble now

    # [Required] [name: logistic_classifier/nn]
    # logistic regression: keyword args of logistic regression classifier of sklearn
    #                      can be passes in params
    # nn: params are optional, can include epochs, batch_size, lr- learning rate, l2- L2 weight penalty
    model:
      name: nn
      params:
          epochs: 1
          batch_size: 50000

    # [Required]
    # k: number of top features to extract
    # aggregation_strategy: stratergies to obtain top features. only mean implemented now
    top_features_stats:
        k: 3000
        aggregation_strategy: mean # [Required][mean/class weighted density/top (k/n_classes) per class]

    # Will store extracted features subset on disk
    # the storage will be according to data=>chunksize parameter
    store_on_disk: True

# MODEL
# DL model to train [Required]
# type: [Linear]
# hyperparameters:
#    layers: [number of genes...number of classes]. eg. [22858, 2048, 64, 6], dropout, activation, weights_init_zero
# start_checkpoint: if resume_from_checkpoint is true, path to checkpoint, note that this will overwrite above hyperparameters and type
# resume_from_checkpoint: to resume training past checkpoint. [BOOLEAN], default: false
model:
    type: 'linear'
    hyperparameters:
        layers: [3000, 6]
        dropout: 0
        weights_init_zero: False
    start_checkpoint: null
    resume_from_checkpoint: False

# TRAINING
# opt: default Adam
# loss_fn: default CrossEntropy
# batch_size: samples per batch during training
# lr: learning rate
# l2: L2 penalty
# epochs: max number of epochs to train
# callbacks: EARLYSTOPPING, MODELCHECKPOINT, TENSORBOARDLOGGING
#     early_stop_patience: max number of epochs for which validation loss does not improve before stopping
#     early_stop_min_delta: minimum increment in validation loss to count as improvement
#     model_checkpoint_interval: intervals of epoch for which model weights are stored
training:
    opt: 'adam'
    loss: 'log'
    batch_size: 5000
    lr: 1.0e-3
    l2: 0
    epochs: 1
    callbacks:
        tensorboard_logging: True
        early_stop:
            patience: 4
            min_delta: 1.0e-4
        model_checkpoint:
            interval: 5

# model_checkpoint: [OPTIONAL IF USING WITH PIPELINE AFTER TRAINING]
#     path_to_model_checkpoint: use model checkpoint to load model
# batch_size: evaluation batch_size
# evaluation_metrics: metrics to perform evaluation available:['accuracy', 'report', 'roc_auc', 'deg']

# deg_config: [Optional] [required if 'deg' in evaluation_metrics]
#     fixed_column: [Required] column name in `adata.obs` containing a fixed condition to subset
#     fixed_condition: [Required] condition to subset data on, belonging to `fixed_column`
#     control_column: Required] column name in `adata.obs` containing the control condition
#     control_conditions: Required] list of conditions in `control_column` to make design matrix for
#     sum_column: Required] column name to sum values across samples
#     fold_change: fold change to filter the differentially expressed genes for volcano plot
#     p_val: p_val to filter the differentially expressed genes for volcano plot
#     y_lim_tuple: values to adjust the Y-axis limits of the plot

# EVALUATION
evaluation:
    model_checkpoint: './scp_test_final_7/best_model'
    batch_size: 5000
    metrics: ['accuracy', 'report', 'roc_auc', 'deg']
    deg_config:
        fixed_column: 'Cell_Type'
        fixed_condition: 'B'
        design_factor: 'Cell_State'
        factor_categories: ['BS1', 'BS2']
        sum_column: 'donor_id'
