{"cells":[{"cell_type":"markdown","id":"a7e5bdf5","metadata":{"id":"a7e5bdf5"},"source":["# <span style=\"color: steelblue;\">Normalization using scaLR</span>"]},{"cell_type":"markdown","id":"0b3cbafc-1741-4535-91c1-75185c5afc08","metadata":{"id":"0b3cbafc-1741-4535-91c1-75185c5afc08"},"source":["Keypoints\n","\n","1. This notebook is designed as a tutorial for using normalization from a scaLR library.\n","2. Also, we have compared results using standard library like sklearn, scanpy for normalization etc.\n","3. These packages are built so to handle very large data say lakhs of samples with low resource constraints, which standard libraries can't handle at once."]},{"cell_type":"markdown","id":"e6012f9b-e45a-4401-9349-0c1fa2a3c81a","metadata":{"id":"e6012f9b-e45a-4401-9349-0c1fa2a3c81a"},"source":["## <span style=\"color: steelblue;\">Cloning scaLR</span>"]},{"cell_type":"code","source":["!git clone https://github.com/infocusp/scaLR.git"],"metadata":{"id":"boKYqxU-QSYV"},"id":"boKYqxU-QSYV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <span style=\"color: steelblue;\">Library Installation and Import</span>"],"metadata":{"id":"3866F5bHNiHC"},"id":"3866F5bHNiHC"},{"cell_type":"code","source":["!pip install anndata scanpy pydeseq2"],"metadata":{"collapsed":true,"id":"WT_923FIQu_H"},"id":"WT_923FIQu_H","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"de5a0712-dff6-4b01-a5cb-f248d944355e","metadata":{"id":"de5a0712-dff6-4b01-a5cb-f248d944355e"},"outputs":[],"source":["from copy import deepcopy\n","import sys\n","sys.path.append('scaLR')\n","\n","import pandas as pd\n","import numpy as np\n","import anndata\n","\n","# scalr library normalization modules.\n","from scalr.data.preprocess import standard_scale, sample_norm\n","from scalr.data_ingestion_pipeline import DataIngestionPipeline\n","from scalr.utils.file_utils import read_data, write_data, write_chunkwise_data\n","\n","# Scanpy library for sample-norm\n","import scanpy as sc\n","# Sklearn library for standard scaler object\n","from sklearn.preprocessing import StandardScaler\n","from os import path\n","\n","%reload_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","source":["## <span style=\"color: steelblue;\">Downloading data</span>\n","- Downloading an anndata from `cellxgene` and making a subset anndata with 1000 genes for the downstream analysis."],"metadata":{"id":"PXTHtcJwN6fk"},"id":"PXTHtcJwN6fk"},{"cell_type":"code","source":["# This shell will take approximately 00:00:24 (hh:mm:ss) to run.\n","!wget -P data https://datasets.cellxgene.cziscience.com/16acb1d0-4108-4767-9615-0b42abe09992.h5ad"],"metadata":{"id":"s3uaC_9bN8dk"},"id":"s3uaC_9bN8dk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading data\n","adata = anndata.read_h5ad('data/16acb1d0-4108-4767-9615-0b42abe09992.h5ad')\n","print(f\"\\nThe anndata has '{adata.n_obs}' cells and '{adata.n_vars}' genes\")"],"metadata":{"id":"GCWWSGSYU1sS"},"id":"GCWWSGSYU1sS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verifying expression values of 1-10th gene in first 10 cells\n","adata.X[:10,:10].A"],"metadata":{"id":"sfoNuTBHVABc"},"id":"sfoNuTBHVABc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- In the current `AnnData` object, the gene expression data in `X` has already been normalized. Ideally, normalization should be applied only if the raw data is present in `X`.\n","- For this tutorial, we will create a new `AnnData` object using the raw gene expression values."],"metadata":{"id":"XsoN2jlLXlNe"},"id":"XsoN2jlLXlNe"},{"cell_type":"code","source":["# Checking for raw gene expression\n","print(f\"Raw expression data in anndata : {adata.raw is not None}\")"],"metadata":{"id":"DhupF7iacYuL"},"id":"DhupF7iacYuL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["adata.raw.X[:10,:10].A"],"metadata":{"id":"mDVcW5TScnch"},"id":"mDVcW5TScnch","execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_adata = anndata.AnnData(X=adata.raw.X,var=adata.var,obs=adata.obs)\n","sc.write('/content/data/raw_adata.h5ad',raw_adata)"],"metadata":{"id":"ju4QSAYbX5Br"},"id":"ju4QSAYbX5Br","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"gzAvbXfeVONf"},"id":"gzAvbXfeVONf"},{"cell_type":"markdown","id":"35ef2c7b-e7ae-44f4-b5e2-429f515d8108","metadata":{"id":"35ef2c7b-e7ae-44f4-b5e2-429f515d8108"},"source":["## <span style=\"color: steelblue;\">Data Generation</span>\n","\n","- In this section, the downloaded anndata will be split into train, validation, and test sets.\n","- To accomplish this, weâ€™ll implement the `generate_train_val_test_split` method in the `DataIngestionPipeline` of scaLR.\n","- We need the required parameters in data config in the form of a dictionary. For more information, please refer to the `DATA CONFIG` section in the [config.yaml](https://github.com/infocusp/scaLR/blob/main/config/config.yaml) file of scaLR.\n"]},{"cell_type":"code","source":["# Parameters of `DataIngestionPipeline`\n","data_config = {'train_val_test': {'full_datapath': '/content/data/16acb1d0-4108-4767-9615-0b42abe09992.h5ad',\n","                                  'splitter_config': {'name': 'GroupSplitter',\n","                                                      'params': {'split_ratio': [7, 1, 2.5],'stratify': 'donor_id'}}},\n","              'target': 'cell_type'}\n","\n","datapath = './data'"],"metadata":{"id":"52RqqHCf5bo1"},"id":"52RqqHCf5bo1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_config"],"metadata":{"id":"a5yZRuSR844S"},"id":"a5yZRuSR844S","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting data\n","data_split = DataIngestionPipeline(data_config=data_config,\n","                                   dirpath = datapath)\n","data_split.generate_train_val_test_split()"],"metadata":{"id":"HvGS6Op85TDI"},"id":"HvGS6Op85TDI","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Verifying `train`, `val`, and `test` data"],"metadata":{"id":"eYtJoocq7rcL"},"id":"eYtJoocq7rcL"},{"cell_type":"code","source":["train_adata = read_data(path.join(datapath, 'train_val_test_split/train.h5ad'))\n","val_adata = read_data(path.join(datapath, 'train_val_test_split/val.h5ad'))\n","test_adata = read_data(path.join(datapath, 'train_val_test_split/test.h5ad'))"],"metadata":{"id":"2Hjjtc5E7kzR"},"id":"2Hjjtc5E7kzR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gene expression data for the first 10 cells and genes in `train.h5ad`.\n","train_adata.X[:10, :10].A"],"metadata":{"id":"goHDw7W-8xBu"},"id":"goHDw7W-8xBu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gene expression data for the first 10 cells and genes in `val.h5ad`.\n","val_adata.X[:10, :10].A"],"metadata":{"id":"J-brkrpQ9zHH"},"id":"J-brkrpQ9zHH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gene expression data for the first 10 cells and genes in `test.h5ad`.\n","test_adata.X[:10, :10].A"],"metadata":{"id":"MCeHgomX94f2"},"id":"MCeHgomX94f2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"5b660df2","metadata":{"collapsed":true,"id":"5b660df2"},"outputs":[],"source":["# Writing train data in chunks to be used with the StandardScaler method in scaLR.\n","write_chunkwise_data(full_data=train_adata,\n","                     sample_chunksize=1000,\n","                     dirpath=path.join(datapath,'train'))"]},{"cell_type":"code","execution_count":null,"id":"d88b5158","metadata":{"id":"d88b5158"},"outputs":[],"source":["# Writing val data in chunks to be used with the StandardScaler method in scaLR.\n","write_chunkwise_data(full_data=val_adata,\n","                     sample_chunksize=1000,\n","                     dirpath=path.join(datapath,'val'))"]},{"cell_type":"code","execution_count":null,"id":"a822b0ac","metadata":{"id":"a822b0ac"},"outputs":[],"source":["# Writing test data in chunks to be used with the StandardScaler method in scaLR.\n","write_chunkwise_data(full_data=test_adata,\n","                     sample_chunksize=1000,\n","                     dirpath=path.join(datapath,'test'))"]},{"cell_type":"markdown","id":"11b34e08-7022-4202-b993-31d33e25e47d","metadata":{"id":"11b34e08-7022-4202-b993-31d33e25e47d"},"source":["## <span style=\"color: steelblue;\">Normalization</span>\n","## <span style=\"color: steelblue;\">1. StandardScaler</span>\n","This method used to normalize the data so that each gene has a mean of 0 and a standard deviation of 1. This standardization balances the data, reducing biases from genes with larger ranges or higher average expression, and improves the consistency of downstream analyses."]},{"cell_type":"markdown","id":"c6053926-c6be-4fac-a32a-5fc0fa2a6f9e","metadata":{"id":"c6053926-c6be-4fac-a32a-5fc0fa2a6f9e"},"source":["### <span style=\"color: steelblue;\">scalr package - how to to use it?</span>"]},{"cell_type":"code","execution_count":null,"id":"00377fe1-fb3e-4558-a492-d5561187a29f","metadata":{"id":"00377fe1-fb3e-4558-a492-d5561187a29f"},"outputs":[],"source":["# Creating object for standard scaling normalization.\n","scalr_std_scaler = standard_scale.StandardScaler(with_mean=False)\n","\n","print('\\n1. `fit()` function parameters :', scalr_std_scaler.fit.__annotations__)\n","print('\\n2. `transform()` function parameters :', scalr_std_scaler.transform.__annotations__)"]},{"cell_type":"code","execution_count":null,"id":"57e7ba69","metadata":{"id":"57e7ba69"},"outputs":[],"source":["# Datapath to store processed_data\n","processed_datapath = './processed_data_ss'"]},{"cell_type":"code","execution_count":null,"id":"92cf9094-9a2c-4f96-a6cf-66f211c4d1f4","metadata":{"scrolled":true,"id":"92cf9094-9a2c-4f96-a6cf-66f211c4d1f4"},"outputs":[],"source":["# Fitting object on train data.\n","## chunk size to process data in chunks - to extract required parameters from data. Enter value that can fit in your memory.\n","## It can be 2k, 3k , 5k, 10k etc...\n","sample_chunksize = 1000\n","scalr_std_scaler.fit(read_data(path.join(datapath, 'train')), sample_chunksize)\n","\n","# Transforming the test data using above created object & storing it at `preprocessed_datapath`.\n","scalr_std_scaler.process_data(read_data(path.join(datapath, 'test')),\n","                                          sample_chunksize,\n","                                          path.join(processed_datapath, 'test'))"]},{"cell_type":"code","execution_count":null,"id":"5d421f92","metadata":{"id":"5d421f92"},"outputs":[],"source":["# Reading transformed test data\n","test_adata_pipeline = read_data(path.join(processed_datapath, 'test'))\n","test_adata_pipeline[:, :].X[:10, :10]"]},{"cell_type":"markdown","id":"36566aec-54ce-4c54-80c9-86a557418f29","metadata":{"id":"36566aec-54ce-4c54-80c9-86a557418f29"},"source":["### <span style=\"color: steelblue;\">sklearn package for standardscaling</span>\n","- Developers can ignore this section"]},{"cell_type":"code","execution_count":null,"id":"a54c31bd-9de2-4772-ba68-405605d65cf4","metadata":{"id":"a54c31bd-9de2-4772-ba68-405605d65cf4"},"outputs":[],"source":["# Standard scaling using sklearn package\n","sklearn_std_scaler = StandardScaler(with_mean=False)\n","sklearn_std_scaler.fit(train_adata.X[:].A)\n","test_adata_sklearn = sklearn_std_scaler.transform(test_adata.X[:].A)\n","test_adata_sklearn[:10, :10]"]},{"cell_type":"markdown","id":"5a592691-0f87-4eb8-aee2-0af44623871e","metadata":{"id":"5a592691-0f87-4eb8-aee2-0af44623871e"},"source":["### <span style=\"color: steelblue;\">Comparing scalr library results with sklearn's library results</span>"]},{"cell_type":"code","execution_count":null,"id":"3b269d5f-976e-4d99-8ae5-6f822fa7c68d","metadata":{"id":"3b269d5f-976e-4d99-8ae5-6f822fa7c68d"},"outputs":[],"source":["# Checking if error is less than 1e-15\n","assert sum(\n","abs(scalr_std_scaler.train_mean[0] -\n","    sklearn_std_scaler.mean_).flatten() < 1e-6\n",") == train_adata.shape[1], \"Train data mean is not correctly calculated...\"\n","\n","assert sum(\n","abs(scalr_std_scaler.train_std[0] - sklearn_std_scaler.scale_).flatten() <\n","1e-6) == train_adata.shape[\n","    1], \"Train data standard deviation is not correctly calculated...\""]},{"cell_type":"markdown","id":"c053b40b-d170-4666-b6ec-b6e1415f2fbb","metadata":{"id":"c053b40b-d170-4666-b6ec-b6e1415f2fbb"},"source":["## <span style=\"color: steelblue;\">2. SampleNorm</span>\n","- In scRNA-seq, each cell may have a different sequencing depth, resulting in some cells having higher total counts (or reads) than others. Normalizing each cell by its total gene count using `SampleNorm` addresses this variability, ensuring consistent expression levels across the dataset and enabling reliable cell-to-cell comparisons.\n","\n","- After normalization, the default sum of gene expression in each cell becomes one. This can be adjusted by specifying a different total using the `scaling_factor` parameter, as in `sample_norm.SampleNorm(scaling_factor='intended sum value')`."]},{"cell_type":"markdown","id":"2def9f30-1d9d-4167-9b18-bb2012b2c6b7","metadata":{"id":"2def9f30-1d9d-4167-9b18-bb2012b2c6b7"},"source":["### <span style=\"color: steelblue;\">scalr package - how to to use it?</span>"]},{"cell_type":"code","execution_count":null,"id":"ebaf5460-f7cd-4cbb-a3f8-502c5fb73861","metadata":{"id":"ebaf5460-f7cd-4cbb-a3f8-502c5fb73861"},"outputs":[],"source":["# Sample norm using pipeline\n","scalr_sample_norm = sample_norm.SampleNorm()\n","\n","print('\\n1. `transform()` function parameters :', scalr_sample_norm.transform.__annotations__)"]},{"cell_type":"code","execution_count":null,"id":"44abc1dd","metadata":{"id":"44abc1dd"},"outputs":[],"source":["# Datapath to store processed_data\n","processed_datapath = './processed_data_sn'"]},{"cell_type":"code","execution_count":null,"id":"18f0b716-ff8d-4ecb-9591-195217f9ac27","metadata":{"id":"18f0b716-ff8d-4ecb-9591-195217f9ac27"},"outputs":[],"source":["# Fitting is not required on train data for sample-norm.\n","sample_chunksize = 1000\n","\n","# Transforming on test data.\n","scalr_sample_norm.process_data(read_data(path.join(datapath, 'test')),\n","                               sample_chunksize,\n","                               path.join(processed_datapath, 'test'))"]},{"cell_type":"code","execution_count":null,"id":"161952ae","metadata":{"id":"161952ae"},"outputs":[],"source":["# Reading transformed test data\n","test_data_sample_norm_pipeline = read_data(path.join(processed_datapath, 'test'))"]},{"cell_type":"markdown","id":"d5b86a86-7e97-4a40-85e3-6558c082c8f2","metadata":{"id":"d5b86a86-7e97-4a40-85e3-6558c082c8f2"},"source":["### <span style=\"color: steelblue;\">Scanpy package for sample-norm</span>\n","- Developers can ignore this section"]},{"cell_type":"code","execution_count":null,"id":"b795b67e","metadata":{"id":"b795b67e"},"outputs":[],"source":["test_adata = read_data(path.join(datapath, 'test'), backed=None)\n","test_adata = test_adata[:, :].to_adata()\n","test_adata"]},{"cell_type":"code","execution_count":null,"id":"0d6bed81-105b-4f66-9c87-ca1ae5f60412","metadata":{"id":"0d6bed81-105b-4f66-9c87-ca1ae5f60412"},"outputs":[],"source":["# Sample norm using scanpy package\n","test_data_sample_norm_sc = sc.pp.normalize_total(test_adata, target_sum=1, inplace=False)\n","test_data_sample_norm_sc['X'][:10, :10].A"]},{"cell_type":"markdown","id":"1ec3be1c-0031-49dc-8783-e1c769baf24d","metadata":{"id":"1ec3be1c-0031-49dc-8783-e1c769baf24d"},"source":["### <span style=\"color: steelblue;\">Comparing scalr library results with scanpy library results</span>"]},{"cell_type":"code","execution_count":null,"id":"ea3e91e9-50c8-49d5-a916-8620975e01d7","metadata":{"id":"ea3e91e9-50c8-49d5-a916-8620975e01d7"},"outputs":[],"source":["# Checking if error is less than 1e-15\n","(abs(test_data_sample_norm_sc['X'] - test_data_sample_norm_pipeline[:, :].X) < 1e-15)[:10, :10]"]},{"cell_type":"code","execution_count":null,"id":"19387599","metadata":{"id":"19387599"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"scalr_minerva","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}